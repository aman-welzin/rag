{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNfr/lTCbayHGv8ePpmY590",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aman-welzin/rag/blob/main/Hybrid_GRAPH_RAG_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRjCF18XN4Y1",
        "outputId": "67ccdf18-59f5-4dc1-e76f-7f18784acf83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.3/312.3 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ollama\n",
            "  Downloading ollama-0.4.8-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting gradio\n",
            "  Downloading gradio-5.27.0-py3-none-any.whl.metadata (16 kB)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade --quiet langchain langchain-community langchain-openai langchain-experimental langchain-neo4j langchain-ollama langchain-huggingface ollama gradio pymupdf transformers chromadb neo4j docling tiktoken yfiles_jupyter_graphs wikipedia json-repair\n",
        "\n",
        "\n",
        "import os\n",
        "!curl https://ollama.ai/install.sh | sh\n",
        "!echo 'debconf debconf/frontend select Noninteractive' | sudo debconf-set-selections\n",
        "!sudo apt-get update && sudo apt-get install -y cuda-drivers\n",
        "os.environ.update({'LD_LIBRARY_PATH': '/usr/lib64-nvidia'})\n",
        "\n",
        "!nvidia-smi\n",
        "\n",
        "!nohup ollama serve &\n",
        "\n",
        "!ollama ps\n",
        "\n",
        "!ollama pull mistral\n",
        "\n",
        "\n",
        "from langchain_core.runnables import (\n",
        "    RunnableBranch,\n",
        "    RunnableLambda,\n",
        "    RunnableParallel,\n",
        "    RunnablePassthrough,\n",
        ")\n",
        "from typing import Tuple, List, Optional\n",
        "\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.prompts.prompt import PromptTemplate\n",
        "from langchain_core.runnables import ConfigurableField\n",
        "from yfiles_jupyter_graphs import GraphWidget\n",
        "from neo4j import GraphDatabase\n",
        "import os\n",
        "from langchain_community.graphs import Neo4jGraph\n",
        "from langchain_community.graphs import Neo4jGraph\n",
        "try:\n",
        "  import google.colab\n",
        "  from google.colab import output\n",
        "  output.enable_custom_widget_manager()\n",
        "except:\n",
        "  pass\n",
        "from langchain_community.vectorstores import Neo4jVector\n",
        "import ollama\n",
        "import gradio as gr\n",
        "from langchain_community.document_loaders import PyMuPDFLoader, TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import OllamaEmbeddings\n",
        "import re\n",
        "from typing import List\n",
        "\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain_community.llms import Ollama\n",
        "from langchain.chains.llm import LLMChain\n",
        "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from transformers import pipeline\n",
        "from google.colab import drive\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from docling.document_converter import DocumentConverter\n",
        "from langchain.schema import Document\n",
        "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
        "from langchain.graphs import Neo4jGraph\n",
        "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
        "from langchain_core.documents import Document\n",
        "from langchain.graphs import Neo4jGraph\n",
        "from langchain_ollama import OllamaLLM\n",
        "import pandas as pd\n",
        "from neo4j import GraphDatabase\n",
        "from yfiles_jupyter_graphs import GraphWidget\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from neo4j_graphrag.retrievers import HybridRetriever\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import Ollama\n",
        "import ast  # Add this line to import the 'ast' module\n",
        "import json\n",
        "from neo4j import GraphDatabase\n",
        "try:\n",
        "  import google.colab\n",
        "  from google.colab import output\n",
        "  output.enable_custom_widget_manager()\n",
        "except:\n",
        "  pass\n",
        "\n",
        "NEO4J_URI=\"neo4j+s://e3cb3331.databases.neo4j.io\"\n",
        "NEO4J_USERNAME=\"neo4j\"\n",
        "NEO4J_PASSWORD=\"RzNdvZMnFteyovdi_A75W5Hj5S8gkd9Hnl26MxmtFdM\"\n",
        "\n",
        "AUTH=(NEO4J_USERNAME, NEO4J_PASSWORD)\n",
        "with GraphDatabase.driver(NEO4J_URI, auth=AUTH) as driver:\n",
        "    driver.verify_connectivity()\n",
        "\n",
        "os.environ[\"NEO4J_URI\"] = NEO4J_URI\n",
        "os.environ[\"NEO4J_USERNAME\"] = NEO4J_USERNAME\n",
        "os.environ[\"NEO4J_PASSWORD\"] = NEO4J_PASSWORD\n",
        "\n",
        "graph = Neo4jGraph()\n",
        "\n",
        "driver = GraphDatabase.driver(\n",
        "    uri=os.environ[\"NEO4J_URI\"],\n",
        "    auth=(os.environ[\"NEO4J_USERNAME\"], os.environ[\"NEO4J_PASSWORD\"])\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def read_files():\n",
        "    print(\"Reading all the files in the directory...\")\n",
        "\n",
        "    csv_path = '/content/cc_feats.csv'\n",
        "\n",
        "    data = pd.read_csv(csv_path)\n",
        "    print(f\"Loaded {len(data)} rows from the CSV file.\")\n",
        "    return data\n",
        "\n",
        "raw_document = read_files()\n",
        "len(raw_document)\n",
        "\n",
        "if isinstance(raw_document, pd.DataFrame):\n",
        "    text_list = []\n",
        "    for _, row in raw_document.iterrows():\n",
        "        row_data = []\n",
        "        for col in raw_document.columns:\n",
        "            value = row[col]\n",
        "            if pd.notna(value):\n",
        "                value = str(value)\n",
        "                row_data.append(f\"{col.replace('_', ' ').capitalize()}: {value}\")\n",
        "        text = \" | \".join(row_data)\n",
        "        text_list.append(text)\n",
        "else:\n",
        "    text_list = [raw_document]\n",
        "\n",
        "documents = [Document(page_content=text) for text in text_list]\n",
        "print(\"printinf documnets\",documents)\n",
        "\n",
        "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=10)\n",
        "# text_chunks = text_splitter.split_documents(documents)\n",
        "# print(f\"Number of chunks: {len(text_chunks)}\")\n",
        "\n",
        "\n",
        "\n",
        "llm = Ollama(model=\"mistral\")\n",
        "llm_transformer = LLMGraphTransformer(llm=llm)\n",
        "\n",
        "\n",
        "graph_documents = llm_transformer.convert_to_graph_documents(documents )\n",
        "\n",
        "graph_documents\n",
        "\n",
        "graph.add_graph_documents(\n",
        "    graph_documents,\n",
        "    baseEntityLabel=True,\n",
        "    include_source=True\n",
        ")\n",
        "\n",
        "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
        "\n",
        "vector_index_similarity = Neo4jVector.from_documents(\n",
        "    documents=text_chunks,\n",
        "    embedding=embedding,\n",
        "    index_name=\"axis_credit_card_index\",\n",
        "    node_label=\"Document\",\n",
        "    text_node_property=\"page_content\",  # property name in text_chunks\n",
        "    embedding_node_property=\"embedding\",\n",
        "    graph=graph\n",
        ")\n",
        "\n",
        "default_cypher = \"MATCH (n) OPTIONAL MATCH (n)-[r]->(m) RETURN n, r, m\"\n",
        "\n",
        "def showGraph(cypher: str = default_cypher):\n",
        "    # create a neo4j session to run queries\n",
        "    driver = GraphDatabase.driver(\n",
        "        uri = os.environ[\"NEO4J_URI\"],\n",
        "        auth = (os.environ[\"NEO4J_USERNAME\"],\n",
        "                os.environ[\"NEO4J_PASSWORD\"]))\n",
        "\n",
        "    session = driver.session()\n",
        "    widget = GraphWidget(graph = session.run(cypher).graph())\n",
        "    widget.node_label_mapping = 'id'\n",
        "    display(widget)\n",
        "    return widget\n",
        "\n",
        "showGraph()\n",
        "\n",
        "\n",
        "vector_index = Neo4jVector.from_existing_graph(\n",
        "    HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\"),\n",
        "    search_type=\"hybrid\",\n",
        "    node_label=\"Document\",\n",
        "    text_node_properties=[\"page_content\"],  # Changed from 'text' to 'page_content'\n",
        "    embedding_node_property=\"embedding\",\n",
        "    graph=graph\n",
        ")\n",
        "\n",
        "def hybrid_graph_rag_qa(query: str, driver):\n",
        "    embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
        "\n",
        "    with driver.session() as session:\n",
        "        session.run(\"\"\"\n",
        "    CREATE FULLTEXT INDEX creditCardFulltext IF NOT EXISTS\n",
        "    FOR (n:Document)\n",
        "    ON EACH [n.page_content]\n",
        "\"\"\")\n",
        "    # Step 2: Setup Hybrid Retriever\n",
        "    retriever = HybridRetriever(\n",
        "        driver=driver,\n",
        "        vector_index_name=\"axis_credit_card_index\",\n",
        "        fulltext_index_name=\"creditCardFulltext\",\n",
        "        embedder=embedder,\n",
        "        return_properties=[\"page_content\"],\n",
        "    )\n",
        "\n",
        "    # Step 3: Fetch relevant documents\n",
        "    retriever_result = retriever.search(query_text=query, top_k=5)\n",
        "    print(\"Retriever Result:\", retriever_result)\n",
        "\n",
        "    if not retriever_result:\n",
        "        print(\"No relevant documents found.\")\n",
        "        return \"No relevant documents found.\"\n",
        "\n",
        "    # Step 4: Prepare the combined context\n",
        "  # Example for when retriever_result is a tuple\n",
        "    # combined_context = \"\"\n",
        "    # for doc in retriever_result:\n",
        "    #     try:\n",
        "    #         # Replace single quotes with double quotes to ensure valid JSON-like structure\n",
        "    #         content_str = doc.content.replace(\"'\", \"\\\"\")\n",
        "\n",
        "    #         # Now use ast.literal_eval safely\n",
        "    #         content_dict = ast.literal_eval(content_str)\n",
        "    #         combined_context += content_dict.get('page_content', '') + \"\\n\\n\"\n",
        "    #     except Exception as e:\n",
        "    #         print(f\"Error parsing content: {e}\")\n",
        "    #         continue\n",
        "    # # combined_context = \"\\n\\n\".join(ast.literal_eval(doc.content)['page_content'] for doc in retriever_result)\n",
        "    # Step 5: Setup the prompt\n",
        "    prompt = PromptTemplate.from_template(\"\"\"\n",
        "You are an intelligent assistant answering questions about credit cards using the **official, pre-extracted, and verified information provided below**.\n",
        "\n",
        "✅ Use **only the context** to answer.\n",
        "❌ Do not say you lack internet access.\n",
        "❌ Do not refer the user to a financial advisor or a website.\n",
        "✅ Do not make assumptions — answer factually, even if partially.\n",
        "\n",
        "---\n",
        "\n",
        "📚 Context:\n",
        "{context}\n",
        "\n",
        "❓ Question:\n",
        "{question}\n",
        "\n",
        "🧠 Answer:\n",
        "\"\"\")\n",
        "\n",
        "    llm = Ollama(model=\"mistral\")\n",
        "    chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "    # Step 7: Run the LLM\n",
        "    response = chain.run({\n",
        "        \"context\": retriever_result,\n",
        "        \"question\": query\n",
        "    })\n",
        "\n",
        "    # Step 8: Print the answer\n",
        "    print(\"🔎 Final Answer:\\n\", response)\n",
        "\n",
        "    # print(\"\\n🧾 Sources:\")\n",
        "    # for i, doc in enumerate(retriever_result):\n",
        "    #     # Access the correct element of the tuple (doc[0] is the actual content)\n",
        "    #     print(f\"\\nSource {i + 1}:\\n{doc[0]}\")\n",
        "\n",
        "    return response\n",
        "hybrid_graph_rag_qa(\"What are the top 3 cards for shopping?\", driver)\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "EG2mB1PVOMyM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}